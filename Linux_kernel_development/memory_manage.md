#### **从需求的角度看内存，是非常简单的，可以总结为两大类:**

<img src="http://10.2.29.14:8089/wwkj/www-x-wowotech-x-net.img.abc188.com/content/uploadfile/201711/fc8934c4c01052c283a2918e2b43e40820171109143731.gif" alt="img" style="zoom:33%;" />

1）CPU有访问内存的需求，包括从内存中取指、从内存中取数据、向内存中写数据。相应的数据流为：

```
CPU<-------------->MMU(Optional)<----------->Memory
```

2）其它外部设备有访问内存的需求，包括从内存“读取”数据、向内存“写入”数据。
这里的“读取”和“写入”加了引号，是因为在大部分情况下，设备不像CPU那样有智能，不具备直接访问内存的能力。总结来说，设备有三种途径访问内存：

	a）由CPU从中中转，数据流如下（本质上是CPU访问内存）： 
	Device<---------------->CPU<--------------------Memory
	
	b）由第三方具有智能的模块（如DMA控制器）中转，数据流如下： 
	Device<----------->DMA Controller<--------->Memory
	
	c）直接访问内存，数据流如下： 
	Device<---------->IOMMU(Optional)<--------->Memory

​		

#### 软件（Linux kernel内存管理模块）的角度看内存

**CPU视角**	

* 看到内存
    关于内存以及内存管理，最初始的需求是：Linux kernel的核心代码（主要包括启动和内存管理），要能看到物理内存。在MMU使能之前，该需求很容易满足。
    但MMU使能之后、Kernel的内存管理机制ready之前，Kernel看到的是虚拟地址，此时需要一些简单且有效的机制，建立虚拟内存到物理内存的映射（可以是部分的映射，但要能够满足kenrel此时的需要）。

* 管理内存
    看到内存之后，下一步就是将它们管理起来。根据不同的内存形态（在物理地址上是否连续、是否具有NUMA内存、是否具有可拔插的内存、等等），可能有不同的管理模型和管理方法。

* 向内核线程/用户进程提供服务
    将内存管理起来之后，就可以向其它人（kernel的其它模块、内核线程、用户空间进程、等等）提供服务了，主要包括： 
    以虚拟地址（VA）的形式，为应用程序提供远大于物理内存的虚拟地址空间（Virtual Address Space） 
    每个进程都有独立的虚拟地址空间，不会相互影响，进而可提供非常好的内存保护（memory protection） 
    提供内存映射（Memory Mapping）机制，以便把物理内存、I/O空间、Kernel Image、文件等对象映射到相应进程的地址空间中，方便进程的访问 
    提供公平、高效的物理内存分配（Physical Memory Allocation）算法 
    提供进程间内存共享的方法（以虚拟内存的形式），也称作Shared Virtual Memory 
    等等

* 更为高级的内存管理需求
    欲望是无止境的，在内存管理模块提供了基础的内存服务之后，Linux系统（包括kernel线程和用户进程）已经可以正常work了，更为高级的需求也产生了，例如： 
    内存的热拔插（memory hotplug） 
    内存的size超过了虚拟地址可寻址的空间怎么办（high memory） 
    超大页（hugetlbpage）的支持 
    利用磁盘作为交换页以扩大可用内存（各种swap机制和算法） 
    在NUMA系统中通过移动物理页面位置的方法提升内存的访问效率（Page migration） 
    内存泄漏的检查 
    内存碎片的整理 
    内存不足时的处理（oom kill机制） 
    等等
    	

**Device视角**

正常情况下，当软件活动只需要CPU参与时（例如简单的数学运算、图像处理等），上面3.1 所涉及内容已经足够了，无论是用户空间程序，还是内核空间程序，都可以欢快的运行了。

不过，当软件操作一些特殊的、可以以自己的方式访问memory的硬件设备的时候，麻烦就出现了：
软件通过CPU视角获得memory，并不能直接被这些硬件设备访问。于是这些硬件设备就提出了需求：

内存管理模块需要为这些设备提供一些特殊的获取内存的接口，这些接口可以按照设备所期望的形式组织内存（因而可以被设备访问），也可以重新映射成CPU视角的形式，以便CPU可以访问。

这就是我们在编写设备驱动的时候会经常遇到的DMA mapping功能，其中DMA是Direct Memory Access的所需，表示（memory）可以被设备直接访问。
	
	

#### Linux kernel从虚拟内存（VM）、DMA mapping以及DMA buffer sharing三个角度，对内存进行管理	

#### <img src="http://10.2.29.14:8089/wwkj/www-x-wowotech-x-net.img.abc188.com/content/uploadfile/201711/c5898a7c238792d61ea6885d5969c09020171109143732.gif" alt="img" style="zoom: 50%;" />

* 其中VM是内存管理的主要模块，也是我们通常意义上所讲的狭义“内存管理”，代码主要分布在mm/以及arch/xxx/mm/两个目录下，其中arch/xxx/mm/*提供平台相关部分的实现，mm/*提供平台无关部分的实现。

* DMA mapping是内存管理的辅助模块，注要提供dma_alloc_xxx（申请可供设备直接访问的内存----dma_addr）和dma_map_xxx（是在CPU视角的虚拟内存和dma_addr之间转换）两类接口。该模块的具体实现依赖于设备访问内存的方式，代码主要分别在drivers/base/*（通用实现）以及arch/xxx/mm/（平台相关的实现）。

* 最后是DMA buffer sharing的机制，用于在不同设备之间共享内存，一般包括两种方法：	

```
传统的、利用CPU虚拟地址中转的方法，例如scatterlist;

dma buffer sharing framework，位于drivers/dma-buf/dma-buf.c中。
```



**用户虚拟地址**

用户空间程序所能看到的常规地址，每个进程有自己的虚拟地址空间



**物理地址**

该地址在处理器和系统内存之间使用



**总线地址**

该地址在外围总线和内存之间使用，通常与处理器使用的物理地址相同，但不是必须的



**内核逻辑地址**

内核逻辑地址组成了内核的常规地址空间，该地址映射了部分（或全部）内存，并经常被视为物理地址，在多数体系结构中，逻辑地址与其相关联的物理地址之间相差一个固定的偏移量，逻辑地址使用硬件内建的指针大小，在32位系统中，无法寻址全部的物理地址，逻辑地址通常保存在unsigned long 或void *这样类型的变量中，kmalloc返回的内存就是内核逻辑地址



**内核虚拟地址**

内核虚拟地址和逻辑地址的相同之处在于，它们都将内核空间的地址映射到物理地址上。内核虚拟地址与物理地址的映射不是线性的、一对一的，逻辑地址才是线性的、一对一的，所有的逻辑地址都是内核虚拟地址，但许多内核虚拟地址不是逻辑地址，vmalloc分配的内存具有一个虚拟地址（但并不存在直接的物理映射），kmap也返回一个虚拟地址，虚拟地址通常保存在指针变量中。	

如果有一个逻辑地址，宏\_\_pa()（在<asm/page.h>中定义）返回其对应的物理地址

使用宏\_\_pa()将物理地址映射到逻辑地址，但只对低端内存有效。



虚拟和物理地址都可以将其分为页号和一个页内的偏移量，如果忽略地址偏移量，并将除去偏移量的剩余位移到右端，该结果为页帧号（PFN）。PAGE_SHIFT指定移动多少位完成这个转换。



**mm**

内存管理中MMU的存在使得进程之间相互隔离，进程访问的空间均为虚拟地址空间 
Linux中用struct mm_struct来描述一个进程的虚拟地址空间，也通常称为内存描述符 
每个进程只有一个mm_struct结构，但可能有多个虚拟内存区间(struct vm_area_struct)，通常用vma表示，不同vma代表着进程空间的各个区域，比如堆、栈、代码区、数据区、各种映射区等等};

![mm](https://cnblogs.gree.com/static/images2018/blog/417887/201809/417887-20180901112152686-1686828268.gif)



**高端内存与低端内存**

内核无法直接操纵未映射到内核地址空间的内存。换句话说，内核对任何内存的访问，都需要使用自己的虚拟地址。只有内存的低端部分（依赖与硬件和内核的配置，一般1到2GB）拥有逻辑地址，剩余的部分（高端内存）没有，在访问特定的高端内存页前，内核必须建立明确的虚拟映射，使该页可在内核地址空间中被访问。因此，许多内核数据结构必须被放置在低端内存中，而高端内存更趋向于为用户空间进程页所保留。

高端内存（>896MB）

是指那些不存在逻辑地址空间的内存，这是因为他们处于内核虚拟地址之上

只有采用特殊的方式进行映射后才能被访问

低端内存（0-896MB）

存在于内核空间之上的逻辑地址内存

常规的可被寻址的内存区域，kmalloc()函数就是在此区域分配内存的



**内存映射和页结构**

内核使用逻辑地址来引用物理内存中的页，但是，增加了高内存支持后，该方法暴露出一个明显的问题-逻辑地址不可用于高内存。因此，处理内存的内核函数越来越多地使用指向`struct page`（在<linux/mm.h>中定义）的指针。该数据结构用于保存内核需要了解的所有物理内存信息；系统上的每个物理页面都有一个`struct page`结构向对应。此结构的某些字段包括以下内容：

atomic_t count;

该页面的引用数量。当计数下降到时 `0`，页面返回到空闲列表。

void *virtual;

如果该页面已被映射，则指向页面的内核虚拟地址；如果未被映射则为NULL。低端内存页始终被映射；高端内存页通常不被映射。此字段并非在所有体系架构中都有；只有在无法轻松计算页的内核虚拟地址的情况下，它才被编译。如果要查看此成员，正确的方法是使用下面讲述的page_address宏。

unsigned long flags;

描述页面状态的一系列标志。其中包括PG_locked，表示页在内存中已被锁定，以及PG_reserved，禁止内存管理系统访问该页。



内核维护一个或多个struct page结构数组，用来跟踪系统中的物理内存。在一些系统中有一个单独的数组称之为mem_map，在另一些系统中，非一致性内存访问（Nonuniform Memory Access，NUMA）系统和有大量不连续物理内存的系统会有多个内存映射数组。



有一些函数和宏用来在page结构指针与虚拟地址之间进行转换：

struct page \*virt_to_page(void \*kaddr)

该宏在<asm/page.h>中定义，负责将内核逻辑地址转换成相应page结构体指针，由于它需要一个逻辑地址，因此不能操作vmalloc生成的地址以及高端内存。

struct page \*pfn_to_page(int pfn)

针对给定的页帧号，返回page结构指针。如果需要的话，在将页帧号传递给pfn_to_page前，使用pfn_valid检查页帧号的合法性

void *page_address(struct page *page);

返回该页的内核虚拟地址（如果存在）。对于高内存，仅当页已被映射时，该地址才存在。在<linux/mm.h>定义。在大多数情况下，使用kmap而不是 page_address。

如果地址存在的话，则返回页的内核虚拟地址。

void *kmap(struct page *page)

void *kunmap(struct page *page)

kmap为系统中的页返回内核虚拟地址。对于低端内存页来说，它只返回页的逻辑地址；对于高端内存，kmap在专用的内核地址空间创建特殊的映射。由kmap创建的映射需要用kunmap释放；对该种映射的数量是有限的，因此不要持有映射过长的时间。kmap调用维护了一个计数器，因此如果两个或多个函数对同一页调用kmap，操作也是正常的，当没有映射的时候，kmap会休眠。

#include <linux/highmem.h>
#include <asm/kmap_types.h>
void *kmap_atomic(struct page *page, enum km_type type)
void kunmap_atomic(void *addr, enum km_type type)

*kmap_atomic*是 *kmap*的高性能形式 。每种体系结构都为原子kmap维护一小段插槽（专用页表项）。*kmap_atomic*的调用者 必须告诉系统在`type`参数中使用哪个插槽。对于驱动程序唯一有意义的插槽是`KM_USER0`和`KM_USER1`（用于直接从用户空间调用中运行的代码），`KM_IRQ0`以及`KM_IRQ1`（用于中断处理程序）。请注意，原子kmap必须原子处理。您的代码在持有代码时无法入睡。还要注意，内核中没有任何东西可以阻止两个功能尝试使用相同的插槽并互相干扰（尽管每个CPU都有一组唯一的插槽）。实际上，争夺原子kmap插槽似乎不是问题。



通过查看/proc/<pid/maps>可以看到进程的内存区域（其中*pid*被进程ID代替）

/proc/iomem 查看设备内存分布




